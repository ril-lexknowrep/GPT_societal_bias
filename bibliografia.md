# Kapcsolódó szakirodalom

## Közvetlenül kapcsolódó cikkek (nyelvmodellek és szociológiai kutatás)
- Schramowski és mtsai 2022. Large pre-trained language models contain human-like biases of what is right and wrong to do. _Nature Machine Intelligence_ 4, 258–268. https://doi.org/10.1038/s42256-022-00458-8 feltöltöttem ide: https://github.com/ril-lexknowrep/GPT_societal_bias/blob/main/cikkek/s42256-022-00458-8.pdf
- Jensen és mtsai 2021. Language Models in Sociological Research: An Application to Classifying Large Administrative Data and Measuring Religiosity. _Sociological Methodology_ 52:1, 30-52. https://doi.org/10.1177/00811750211053370 Preprint letölthető itt: https://www.researchgate.net/publication/355549483_Language_Models_in_Sociological_Research_An_Application_to_Classifying_Large_Administrative_Data_and_Measuring_Religiosity/link/6177a1270be8ec17a9305dbc/download

## Nyelvmodellek és társadalmi előítéletek
- Anoop és mtsai 2022. Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias. In Mathew és mtsai (szerk.),  _Responsible Data Science. Select Proceedings of ICDSE 2021 (LNEE 940),_ 13-45. A könyv letölthető itt: http://libgen.is/book/index.php?md5=52B967E068576EBAACBF45C49FC005A2
  **Ez egy fontos cikk, egy csomó szakirodalmat áttekint a korábbi évekből, amely társadalmi előítéleteknek a gépi tanulással alkotott nyelvtechnológiai termékekben való tükröződésével és ennek kezelésével foglalkoztak, ezért képet alkothatunk belőle a kérdés jelenlegi állásáról.**
- Hovy és Prabhumoye 2021. Five sources of bias in natural language processing. _Language and Linguistics Compass_ 15, e12432. https://doi.org/10.1111/lnc3.12432 **Ez is az előzőhöz hasonló áttekintő cikk.**
- Kaneko és Bollegala 2022. Unmasking the Mask – Evaluating Social Biases in Masked Language Models. _Proceedings of the AAAI 2022,_ 11954-11962. https://doi.org/10.1609/aaai.v36i11.21453
- Akyürek 2022. Challenges in Measuring Bias via Open-Ended Language Generation. _Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing 2022,_ 76. https://aclanthology.org/2022.gebnlp-1.9/
- Orgad és Belinkov 2022. Choose Your Lenses: Flaws in Gender Bias Evaluation. _Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing 2022,_ 151-167. https://aclanthology.org/2022.gebnlp-1.17/
- Touileb és mtsai 2022. Occupational Biases in Norwegian and Multilingual Language Models. _Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing 2022.,_ 200-211. https://aclanthology.org/2022.gebnlp-1.21/
- Takeshita és mtsai 2022. Speciesist language and nonhuman animal bias in English Masked Language Models. Information Processing & Management 59, 103050. https://doi.org/10.1016/j.ipm.2022.103050
- Das és Balke 2022. Quantifying Bias from Decoding Techniques in Natural Language Generation. _Proceedings of COLING 29,_ 1311–1323. https://aclanthology.org/2022.coling-1.112/
- Venkit és mtsai 2022. A Study of Implicit Bias in Pretrained Language Models against People with Disabilities. _Proceedings of COLING 29,_ 1324–1332. https://aclanthology.org/2022.coling-1.113/
- Nozza és mtsai 2022. Measuring Harmful Sentence Completion in Language Models for LGBTQIA+ Individuals. _Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion,_ 26-34. https://aclanthology.org/2022.ltedi-1.4/
- Parrish és mtsai 2022. BBQ: A hand-built bias benchmark for question answering. _Findings of the Association for Computational Linguistics: ACL 2022,_ 2086-2105. https://aclanthology.org/2022.findings-acl.165/
- Nozza és mtsai 2022. Pipelines for Social Bias Testing of Large Language Models. _Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models,_ 68-74. https://aclanthology.org/2022.bigscience-1.6/
- Névéol és mtsai 2022. French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English. _Proceedings of ACL 60 (Volume 1: Long Papers),_ 8521–8531. https://aclanthology.org/2022.acl-long.583/
- Holtermann és mtsai 2022. Fair and Argumentative Language Modeling for Computational Argumentation. _Proceedings of ACL 60 (Volume 1: Long Papers),_ 7841–7861. https://aclanthology.org/2022.acl-long.541/
- Zhou és mtsai 2022. Sense Embeddings are also Biased – Evaluating Social Biases in Static and Contextualised Sense Embeddings. _Proceedings of ACL 60 (Volume 1: Long Papers),_ 1924–1935. https://aclanthology.org/2022.acl-long.135/
- Dev és mtsai 2022. On Measures of Biases and Harms in NLP. _Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022,_ 246-267. https://aclanthology.org/2022.findings-aacl.24/
- Bhatt és mtsai 2022. Re-contextualizing Fairness in NLP: The Case of India. _Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),_ 727-740. https://aclanthology.org/2022.aacl-main.55/
- Kraft és Usbeck 2022. The Lifecycle of “Facts”: A Survey of Social Bias in Knowledge Graphs. _Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),_ 639-652. https://aclanthology.org/2022.aacl-main.49/
- Lorentzen 2022. _Social Biases in Language Models: Gender Stereotypes in GPT-3 Generated Stories._ Vizsgadolgozat, Uppsalai Egyetem. http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1696604
- Liang és mtsai 2021. Towards Understanding and Mitigating Social Biases in Language Models. _arXiv._ https://arxiv.org/abs/2106.13219
- Delobelle és mtsai 2021. Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models. _arXiv._ https://arxiv.org/abs/2112.07447
- Caliskan 2021. _Detecting and mitigating bias in natural language processing._ https://www.brookings.edu/research/detecting-and-mitigating-bias-in-natural-language-processing/
- Nangia és mtsai 2020. CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models. In _Proceedings of EMNLP 2020,_ 1953-1967. https://aclanthology.org/2020.emnlp-main.154/
- Friedman és mtsai 2019. Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis. Proceedings of the First Workshop on Gender Bias in Natural Language Processing 18-24. https://aclanthology.org/W19-3803/
- Kurita és mtsai 2019. Measuring Bias in Contextualized Word Representations. Proceedings of the First Workshop on Gender Bias in Natural Language Processing 166-172. https://aclanthology.org/W19-3823/

## Lazán kapcsolódó cikkek
- Zhao és mtsai 2022. From Polarity to Intensity: Mining Morality from Semantic Space. _Proceedings of COLING 29,_ 1250–1262. https://aclanthology.org/2022.coling-1.107/
- Elsafoury és mtsai 2022. SOS: Systematic Offensive Stereotyping Bias in Word Embeddings. _Proceedings of COLING 29,_ 1263-1274. https://aclanthology.org/2022.coling-1.108/
- Sha és mtsai 2022. Bigger Data or Fairer Data? Augmenting BERT via Active Sampling for Educational Text Classification. _Proceedings of COLING 29,_ 1275-1285. https://aclanthology.org/2022.coling-1.109/
- Kaneko és mtsai 2022. Debiasing Isn’t Enough! – on the Effectiveness of Debiasing MLMs and Their Social Biases in Downstream Tasks. _Proceedings of COLING 29,_ 1299–1310. https://aclanthology.org/2022.coling-1.111/
- Shen és mtsai 2022. Social Norms-Grounded Machine Ethics in Complex Narrative Situation. _Proceedings of COLING 29,_ 1333–1343. https://aclanthology.org/2022.coling-1.114/
- Izzidien 2022. Word vector embeddings hold social ontological relations capable of reflecting meaningful fairness assessments. _AI & Society_ 37, 299–318. https://link.springer.com/article/10.1007/s00146-021-01167-3
- Gira és mtsai 2022. Debiasing Pre-Trained Language Models via Efficient Fine-Tuning. _Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion,_ 59-69. https://aclanthology.org/2022.ltedi-1.8/
- Park és Rudzicz 2022. Detoxifying Language Models with a Toxic Corpus. _Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion,_ 41-46. https://aclanthology.org/2022.ltedi-1.6/
- Khalid és mtsai 2022. Suum Cuique: Studying Bias in Taboo Detection with a Community Perspective. _Findings of the Association for Computational Linguistics: ACL 2022,_ 2883-2896. https://aclanthology.org/2022.findings-acl.227/
- Meade és mtsai 2022. An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models. _Proceedings of ACL 60 (Volume 1: Long Papers),_ 1878–1898. https://aclanthology.org/2022.acl-long.132/
- Hertzberg és mtsai 2022. Distributional properties of political dogwhistle representations in Swedish BERT. _Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH),_ 170-175. https://aclanthology.org/2022.woah-1.16/
- Kumar és mtsai 2021. An Overview of Fairness in Data – Illuminating the Bias in Data Pipeline. _Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion,_ 34-45. https://aclanthology.org/2021.ltedi-1.5/

## Lazán kapcsolódó konferenciák
- Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing. 2022. https://aclanthology.org/volumes/2022.gebnlp-1/
- Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing. 2021. https://aclanthology.org/volumes/2021.gebnlp-1/
- Proceedings of the Second Workshop on Gender Bias in Natural Language Processing. 2020. https://aclanthology.org/volumes/2020.gebnlp-1/
- Proceedings of the First Workshop on Gender Bias in Natural Language Processing. 2019. https://aclanthology.org/volumes/W19-38/
