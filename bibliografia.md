# Kapcsolódó szakirodalom

## Közvetlenül kapcsolódó cikkek (nyelvmodellek és szociológiai kutatás)
- Schramowski és mtsai 2022. Large pre-trained language models contain human-like biases of what is right and wrong to do. _Nature Machine Intelligence_ 4, 258–268. https://doi.org/10.1038/s42256-022-00458-8 feltöltöttem ide: https://github.com/ril-lexknowrep/GPT_societal_bias/blob/main/cikkek/s42256-022-00458-8.pdf
- Jensen és mtsai 2021. Language Models in Sociological Research: An Application to Classifying Large Administrative Data and Measuring Religiosity. _Sociological Methodology_ 52:1, 30-52. https://doi.org/10.1177/00811750211053370 Preprint letölthető itt: https://www.researchgate.net/publication/355549483_Language_Models_in_Sociological_Research_An_Application_to_Classifying_Large_Administrative_Data_and_Measuring_Religiosity/link/6177a1270be8ec17a9305dbc/download

## Nyelvmodellek és társadalmi előítéletek
- Anoop és mtsai 2022. Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias. In Mathew és mtsai (szerk.),  _Responsible Data Science. Select Proceedings of ICDSE 2021 (LNEE 940),_ 13-45. A könyv letölthető itt: http://libgen.is/book/index.php?md5=52B967E068576EBAACBF45C49FC005A2
  **Ez egy fontos cikk, egy csomó szakirodalmat áttekint a korábbi évekből, amely társadalmi előítéleteknek a gépi tanulással alkotott nyelvtechnológiai termékekben való tükröződésével és ennek kezelésével foglalkoztak, ezért képet alkothatunk belőle a kérdés jelenlegi állásáról.**
- Hovy és Prabhumoye 2021. Five sources of bias in natural language processing. _Language and Linguistics Compass_ 15, e12432. https://doi.org/10.1111/lnc3.12432 **Ez is az előzőhöz hasonló áttekintő cikk.**
- Kasirzadeh és Gabriel 2022. In conversation with AI: building better language models. DeepMind technical blog. https://www.deepmind.com/blog/in-conversation-with-ai-building-better-language-models
- Kaneko és Bollegala 2022. Unmasking the Mask – Evaluating Social Biases in Masked Language Models. _Proceedings of the AAAI 2022,_ 11954-11962. https://doi.org/10.1609/aaai.v36i11.21453
- Akyürek és mtsai 2022. Challenges in Measuring Bias via Open-Ended Language Generation. _Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing 2022,_ 76. https://aclanthology.org/2022.gebnlp-1.9/ Itt csak egy összefoglaló van belinkelve, a tulajdonképpeni cikk itt van: https://arxiv.org/abs/2205.11601 **Flórián szerint ez egy tök jó cikk, "A toxity score ebben olyasmi, amire én is gondoltam. Lehetne izgalmasabb kódolást csinálni, ami nem csak ezen az egy dimenzión méri (pozitív vagy negatív) hanem thematic analysis-szel több dimenziót mérhetne."** _Gergő szerint a fontosabb tanulságai: nincs jó módszertan, ti. a toxikusság mérésére a korábbi (tavalyi) szakirodalomban javasolt eszközök egymásnak teljesen ellentmondó eredményeket adnak, és a GPT-modell véletlenszám-generátorának beállításaitól függően ugyanaz a mérőeszköz is drasztikusan eltérő eredményeket ad ugyanarra a modellre, de ezek az eredmények teljesen összevissza jelentkeznek, bármi előrejelezhető, szisztematikus mintázat nélkül. Tehát például a 30 hosszúságú folytatás azonos beállítások mellett sokkal toxikusabbra jön ki, mint a 10 hosszúságú, de kevésbé toxikusra, mint az 50 hosszúságú. Emellett alapvető koncepcionális problémákat is azonosítanak, pl. eszköztől függ és részben önkényes, hogy mit azonosítunk és mérünk toxikusként; a promptra adott folytatás toxikusságának sokszor semmi köze a prompthoz, mert a nyelvmodelll elkalandozik, pl. férfiról mond csúnyát annak ellenére, hogy a promptban nőről kérdezzük. Tehát szerintem tényleg tanulságos az előadás és a cikk, de nem úgy, hogy át tudnánk közvetlenül venni a módszertanukat, hacsak nem azt akarjuk reprodukálni a magyarra, hogy ott se működik. De az angolra javasolt promptkészletet megpróbálhatjuk adaptálni a magyarra._
- Orgad és Belinkov 2022. Choose Your Lenses: Flaws in Gender Bias Evaluation. _Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing 2022,_ 151-167. https://aclanthology.org/2022.gebnlp-1.17/
- Touileb és mtsai 2022. Occupational Biases in Norwegian and Multilingual Language Models. _Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing 2022.,_ 200-211. https://aclanthology.org/2022.gebnlp-1.21/
- Takeshita és mtsai 2022. Speciesist language and nonhuman animal bias in English Masked Language Models. _Information Processing & Management_ 59, 103050. https://doi.org/10.1016/j.ipm.2022.103050
- Das és Balke 2022. Quantifying Bias from Decoding Techniques in Natural Language Generation. _Proceedings of COLING 29,_ 1311–1323. https://aclanthology.org/2022.coling-1.112/
- Venkit és mtsai 2022. A Study of Implicit Bias in Pretrained Language Models against People with Disabilities. _Proceedings of COLING 29,_ 1324–1332. https://aclanthology.org/2022.coling-1.113/
- Nozza és mtsai 2022. Measuring Harmful Sentence Completion in Language Models for LGBTQIA+ Individuals. _Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion,_ 26-34. https://aclanthology.org/2022.ltedi-1.4/
- Parrish és mtsai 2022. BBQ: A hand-built bias benchmark for question answering. _Findings of the Association for Computational Linguistics: ACL 2022,_ 2086-2105. https://aclanthology.org/2022.findings-acl.165/
- Nozza és mtsai 2022. Pipelines for Social Bias Testing of Large Language Models. _Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models,_ 68-74. https://aclanthology.org/2022.bigscience-1.6/
- Névéol és mtsai 2022. French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English. _Proceedings of ACL 60 (Volume 1: Long Papers),_ 8521–8531. https://aclanthology.org/2022.acl-long.583/
- Holtermann és mtsai 2022. Fair and Argumentative Language Modeling for Computational Argumentation. _Proceedings of ACL 60 (Volume 1: Long Papers),_ 7841–7861. https://aclanthology.org/2022.acl-long.541/
- Zhou és mtsai 2022. Sense Embeddings are also Biased – Evaluating Social Biases in Static and Contextualised Sense Embeddings. _Proceedings of ACL 60 (Volume 1: Long Papers),_ 1924–1935. https://aclanthology.org/2022.acl-long.135/
- Dev és mtsai 2022. On Measures of Biases and Harms in NLP. _Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022,_ 246-267. https://aclanthology.org/2022.findings-aacl.24/
- Bhatt és mtsai 2022. Re-contextualizing Fairness in NLP: The Case of India. _Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),_ 727-740. https://aclanthology.org/2022.aacl-main.55/
- Kraft és Usbeck 2022. The Lifecycle of “Facts”: A Survey of Social Bias in Knowledge Graphs. _Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),_ 639-652. https://aclanthology.org/2022.aacl-main.49/
- Lorentzen 2022. _Social Biases in Language Models: Gender Stereotypes in GPT-3 Generated Stories._ Vizsgadolgozat, Uppsalai Egyetem. http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1696604
- Liang és mtsai 2021. Towards Understanding and Mitigating Social Biases in Language Models. _arXiv._ https://arxiv.org/abs/2106.13219
- Delobelle és mtsai 2021. Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models. _arXiv._ https://arxiv.org/abs/2112.07447
- Caliskan 2021. _Detecting and mitigating bias in natural language processing._ https://www.brookings.edu/research/detecting-and-mitigating-bias-in-natural-language-processing/
- Bender és mtsai 2021. On the dangers of stochastic parrots: Can language models be too big. _Proceedings of FAccT '21,_ 610-623. https://dl.acm.org/doi/10.1145/3442188.3445922 **Ez egy sokat hivatkozott cikk.**
- Dhamala és mtsai 2021. BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation. _Proceedings of FAccT '21,_ 862-872. https://dl.acm.org/doi/10.1145/3442188.3445924 **Ez az egyik előítéletességmérő eszköz, amit Akyürek és mtsai 2022 használ.**
- McGuffie és Newhouse 2020. The Radicalization Risks of GPT-3 and Advanced Neural Language Models. _arXiv._ https://arxiv.org/abs/2009.06807
- Gehman és mtsai 2020. RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models. _arXiv._ https://arxiv.org/abs/2009.11462
- Nangia és mtsai 2020. CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models. In _Proceedings of EMNLP 2020,_ 1953-1967. https://aclanthology.org/2020.emnlp-main.154/
- Friedman és mtsai 2019. Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis. _Proceedings of the First Workshop on Gender Bias in Natural Language Processing_ 18-24. https://aclanthology.org/W19-3803/
- Kurita és mtsai 2019. Measuring Bias in Contextualized Word Representations. _Proceedings of the First Workshop on Gender Bias in Natural Language Processing_ 166-172. https://aclanthology.org/W19-3823/
- Gonen és Goldberg 2019. Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them. _Proceedings of the 2019 Workshop on Widening NLP_ 60-63. https://aclanthology.org/W19-3621/

## Lazán kapcsolódó cikkek
- Zhao és mtsai 2022. From Polarity to Intensity: Mining Morality from Semantic Space. _Proceedings of COLING 29,_ 1250–1262. https://aclanthology.org/2022.coling-1.107/
- Elsafoury és mtsai 2022. SOS: Systematic Offensive Stereotyping Bias in Word Embeddings. _Proceedings of COLING 29,_ 1263-1274. https://aclanthology.org/2022.coling-1.108/
- Sha és mtsai 2022. Bigger Data or Fairer Data? Augmenting BERT via Active Sampling for Educational Text Classification. _Proceedings of COLING 29,_ 1275-1285. https://aclanthology.org/2022.coling-1.109/
- Kaneko és mtsai 2022. Debiasing Isn’t Enough! – on the Effectiveness of Debiasing MLMs and Their Social Biases in Downstream Tasks. _Proceedings of COLING 29,_ 1299–1310. https://aclanthology.org/2022.coling-1.111/
- Shen és mtsai 2022. Social Norms-Grounded Machine Ethics in Complex Narrative Situation. _Proceedings of COLING 29,_ 1333–1343. https://aclanthology.org/2022.coling-1.114/
- Izzidien 2022. Word vector embeddings hold social ontological relations capable of reflecting meaningful fairness assessments. _AI & Society_ 37, 299–318. https://link.springer.com/article/10.1007/s00146-021-01167-3
- Gira és mtsai 2022. Debiasing Pre-Trained Language Models via Efficient Fine-Tuning. _Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion,_ 59-69. https://aclanthology.org/2022.ltedi-1.8/
- Park és Rudzicz 2022. Detoxifying Language Models with a Toxic Corpus. _Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion,_ 41-46. https://aclanthology.org/2022.ltedi-1.6/
- Khalid és mtsai 2022. Suum Cuique: Studying Bias in Taboo Detection with a Community Perspective. _Findings of the Association for Computational Linguistics: ACL 2022,_ 2883-2896. https://aclanthology.org/2022.findings-acl.227/
- Meade és mtsai 2022. An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models. _Proceedings of ACL 60 (Volume 1: Long Papers),_ 1878–1898. https://aclanthology.org/2022.acl-long.132/
- Hertzberg és mtsai 2022. Distributional properties of political dogwhistle representations in Swedish BERT. _Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH),_ 170-175. https://aclanthology.org/2022.woah-1.16/
- Weidinger és mtsai 2022. Taxonomy of Risks posed by Language Models. _Proceedings of FAccT '22,_ 214-229. https://dl.acm.org/doi/10.1145/3531146.3533088
- Kumar és mtsai 2021. An Overview of Fairness in Data – Illuminating the Bias in Data Pipeline. _Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion,_ 34-45. https://aclanthology.org/2021.ltedi-1.5/
- Hutson 2021. Robo-writers: the rise and risks of language-generating AI. _Nature_ 591, 22-25. https://doi.org/10.1038/d41586-021-00530-0 
- Jacobs és Wallach 2021. Measurement and Fairness. _Proceedings of FAccT '21,_ 375-385. https://dl.acm.org/doi/10.1145/3442188.3445901 **Jól megírt cikk. Azt foglalja össze, hogy milyen problémák merülnek fel közvetlenül nem megfigyelhető, elméleti fogalmak mérésével kapcsolatban. Bevezeti többek között az essentially contested concept fogalmát. Hivatkozható.**
- Yang és Roberts 2021. Censorship of Online Encyclopedias: Implications for NLP Models. _Proceedings of FAccT '21,_ 537-548. **Arról szól, hogy két különböző (történetesen egy államilag cenzúrázott és egy viszonylag szabad) korpuszon tanított klasszikus szóvektorok közötti különbségek mérhetően korrelálnak a pártállam ideológiai preferenciáival. Nem túl izgalmas, de annyiban releváns nekünk, hogy azt vizsgálja, hogy inkább pozitív vagy inkább negatív-e a viszonyulása a lexikális modelleknek olyan fogalmakhoz, mint a szabadság, a demokrácia vagy a megfigyelés és az ellenőrzés.**

## Lazán kapcsolódó konferenciák
- Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing. 2022. https://aclanthology.org/volumes/2022.gebnlp-1/
- Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing. 2021. https://aclanthology.org/volumes/2021.gebnlp-1/
- Proceedings of the Second Workshop on Gender Bias in Natural Language Processing. 2020. https://aclanthology.org/volumes/2020.gebnlp-1/
- Proceedings of the First Workshop on Gender Bias in Natural Language Processing. 2019. https://aclanthology.org/volumes/W19-38/
